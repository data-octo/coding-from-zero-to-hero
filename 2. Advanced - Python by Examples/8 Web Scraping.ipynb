{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Web Scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Web Scraping?\n",
    "\n",
    "The dictionary meaning of word ‘Scrapping’ implies getting something from the web. Here two questions arise: What we can get from the web and How to get that.\n",
    "\n",
    "The answer to the first question is ‘data’. Data is indispensable for any programmer and the basic requirement of every programming project is the large amount of useful data.\n",
    "\n",
    "The answer to the second question is a bit tricky, because there are lots of ways to get data. In general, we may get data from a database or data file and other sources. But what if we need large amount of data that is available online? One way to get such kind of data is to manually search (clicking away in a web browser) and save (copy-pasting into a spreadsheet or file) the required data. This method is quite tedious and time consuming. Another way to get such data is using web scraping.\n",
    "\n",
    "Web scraping, also called web data mining or web harvesting, is the process of constructing an agent which can extract, parse, download and organize useful information from the web automatically. In other words, we can say that instead of manually saving the data from websites, the web scraping software will automatically load and extract data from multiple websites as per our requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup\n",
    "\n",
    "Beautiful Soup is a Python library for getting data out of HTML, XML, and other markup languages. Say you’ve found some webpages that display data relevant to your research, such as date or address information, but that do not provide any way of downloading the data directly. Beautiful Soup helps you pull particular content from a webpage, remove the HTML markup, and save the information. It is a tool for web scraping that helps you clean up and parse the documents you have pulled down from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--------------------------------------------\nThe Help (2011)\nAn aspiring author during the civil rights movement of the 1960s decides to write a book detailing the African American maids' point of view on the white families for which they work, and the hardships they go through on a daily basis.\n--------------------------------------------\nGreen Book (2018)\nA working-class Italian-American bouncer becomes the driver of an African-American classical pianist on a tour of venues through the 1960s American South.\n--------------------------------------------\nDouble Indemnity (1944)\nAn insurance representative lets himself be talked by a seductive housewife into a murder/insurance fraud scheme that arouses the suspicion of an insurance investigator.\n--------------------------------------------\nLa haine (1995)\n24 hours in the lives of three young men in the French suburbs the day after a violent riot.\n--------------------------------------------\nReservoir Dogs (1992)\nWhen a simple jewelry heist goes horribly wrong, the surviving criminals begin to suspect that one of them is a police informant.\n--------------------------------------------\nRelatos salvajes (2014)\nSix short stories that explore the extremities of human behavior involving people in distress.\n--------------------------------------------\nInto the Wild (2007)\nAfter graduating from Emory University, top student and athlete Christopher McCandless abandons his possessions, gives his entire $24,000 savings account to charity and hitchhikes to Alaska to live in the wilderness. Along the way, Christopher encounters a series of characters that shape his life.\n--------------------------------------------\n12 Years a Slave (2013)\nIn the antebellum United States,\n--------------------------------------------\nMononoke-hime (1997)\nOn a journey to find the cure for a Tatarigami's curse, Ashitaka finds himself in the middle of a war between the forest gods and Tatara, a mining colony. In this quest he also meets San, the Mononoke Hime.\n--------------------------------------------\n3 Idiots (2009)\nTwo friends are searching for their long lost companion. They revisit their college days and recall the memories of their friend who inspired them to think differently, even as the rest of the world called them \"idiots\".\n--------------------------------------------\n"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "def get_imd_movies(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    movies = soup.find_all(\"td\", class_=\"titleColumn\")\n",
    "    random.shuffle(movies)\n",
    "    return movies\n",
    "def get_imd_summary(url):\n",
    "    movie_page = requests.get(url)\n",
    "    soup = BeautifulSoup(movie_page.text, 'html.parser')\n",
    "    return soup.find(\"div\", class_=\"summary_text\").contents[0].strip()\n",
    "\n",
    "def get_imd_movie_info(movie):\n",
    "    movie_title = movie.a.contents[0]\n",
    "    movie_year = movie.span.contents[0]\n",
    "    movie_url = 'http://www.imdb.com' + movie.a['href']\n",
    "    return movie_title, movie_year, movie_url\n",
    "\n",
    "def imd_movie_picker():\n",
    "    ctr=0\n",
    "    print(\"--------------------------------------------\")\n",
    "    for movie in get_imd_movies('http://www.imdb.com/chart/top'):\n",
    "        movie_title, movie_year, movie_url = get_imd_movie_info(movie)\n",
    "        movie_summary = get_imd_summary(movie_url)\n",
    "        print(movie_title, movie_year)\n",
    "        print(movie_summary)\n",
    "        print(\"--------------------------------------------\")\n",
    "        ctr=ctr+1\n",
    "        if (ctr==10):\n",
    "          break;   \n",
    "if __name__ == '__main__':\n",
    "    imd_movie_picker()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy \n",
    "\n",
    "Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.\n",
    "\n",
    "Even though Scrapy was originally designed for web scraping, it can also be used to extract data using APIs (such as Amazon Associates Web Services) or as a general purpose web crawler.\n",
    "\n",
    "** Walk-through of an example spider **\n",
    "\n",
    "In order to show you what Scrapy brings to the table, we’ll walk you through an example of a Scrapy Spider using the simplest way to run a spider.\n",
    "\n",
    "Here’s the code for a spider that scrapes famous quotes from website http://quotes.toscrape.com, following the pagination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/tag/humor/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'author': quote.xpath('span/small/text()').get(),\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(\"href\")').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put this in a text file, name it to something like quotes_spider.py and run the spider using the runspider command:\n",
    "```\n",
    "scrapy runspider quotes_spider.py -o quotes.json\n",
    "```\n",
    "When this finishes you will have in the quotes.json file a list of the quotes in JSON format, containing text and author, looking like this (reformatted here for better readability):\n",
    "```\n",
    "[{\n",
    "    \"author\": \"Jane Austen\",\n",
    "    \"text\": \"\\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\\u201d\"\n",
    "},\n",
    "{\n",
    "    \"author\": \"Groucho Marx\",\n",
    "    \"text\": \"\\u201cOutside of a dog, a book is man's best friend. Inside of a dog it's too dark to read.\\u201d\"\n",
    "},\n",
    "{\n",
    "    \"author\": \"Steve Martin\",\n",
    "    \"text\": \"\\u201cA day without sunshine is like, you know, night.\\u201d\"\n",
    "},\n",
    "...]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda8cbf85cc44b94ed2a5c37d93e746daff",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}